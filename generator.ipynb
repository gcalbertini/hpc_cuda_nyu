{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "destination = \"../generated_data\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given functions\n",
    "def get_d(deg_true):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "\n",
    "    Returns:\n",
    "    a: (np array of size (deg_true + 1)) coefficients of polynomial g\n",
    "    \"\"\"\n",
    "    return 5 * np.random.randn(deg_true + 1)\n",
    "\n",
    "\n",
    "def get_design_mat(x, deg):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x: (np.array of size N)\n",
    "    deg: (int) max degree used to generate the design matrix\n",
    "\n",
    "    Returns:\n",
    "    X: (np.array of size N x (deg_true + 1)) design matrix\n",
    "    \"\"\"\n",
    "    X = np.array([x ** i for i in range(deg + 1)]).T\n",
    "    return X\n",
    "\n",
    "\n",
    "def draw_sample(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.sort(np.random.rand(N))\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def draw_sample_with_noise(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.random.rand(N)\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a + np.random.randn(N)\n",
    "    return x, y\n",
    "\n",
    "# Define our least squares estimator function\n",
    "\n",
    "\n",
    "def least_squares_estimator(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "\n",
    "    Returns:\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    \"\"\"\n",
    "    # Make sure N > d\n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        raise ValueError(\"You must have at least as many rows as columns!\")\n",
    "    else:\n",
    "        # Compute the solution for b using the closed form linear algebra solution\n",
    "        b_hat = np.linalg.inv(X.T@X) @ X.T @ y\n",
    "        return b_hat\n",
    "\n",
    "\n",
    "def empirical_risk(X, y, b_hat):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    Returns:\n",
    "    emp_risk: (float) \n",
    "    \"\"\"\n",
    "    # Get # of observations\n",
    "    N = X.shape[0]\n",
    "    # Calculate Predictions\n",
    "    y_hat = X @ b_hat\n",
    "    # Calculate squared errors and then empirical risk\n",
    "    sum_of_squared_errors = sum((y_hat-y)**2)\n",
    "    emp_risk = sum_of_squared_errors / N\n",
    "    emp_risk = emp_risk / 2  # because we have 1/2 in our loss function\n",
    "    return emp_risk\n",
    "\n",
    "\n",
    "def noisy_emp_and_gen_risk(d, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    d: (int) degree of polynomial desired\n",
    "    n: (int) number of samples to be generated in \n",
    "\n",
    "    Outputs:\n",
    "    training_error: (float) average sum of squares of loss function on training data\n",
    "    generalization_error: (float) average Sum of Squares of loss function on test data\n",
    "    \"\"\"\n",
    "    # Generate design matrices\n",
    "    X_train = get_design_mat(x_train, d)\n",
    "    X_test = get_design_mat(x_test, d)\n",
    "\n",
    "    # Calculate b_hat\n",
    "    b_hat = least_squares_estimator(X_train, y_train)\n",
    "    training_error = empirical_risk(X_train, y_train, b_hat)\n",
    "    generalization_error = empirical_risk(X_test, y_test, b_hat)\n",
    "\n",
    "    return training_error, generalization_error, b_hat\n",
    "\n",
    "\n",
    "def poly_risk_gen(d, n, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Ouptut:\n",
    "    train_error_arr: (np.array) e_t for various n\n",
    "    test_error_arr: (np.array) e_g for various n\n",
    "    \"\"\"\n",
    "    # Initiliaze np arrays\n",
    "    train_error_arr = []\n",
    "    test_error_arr = []\n",
    "\n",
    "    # Iterate over N\n",
    "    for i in range(d+1, len(n)):\n",
    "\n",
    "        # Get relevant subset of data\n",
    "        train_x_subset = x_train[:i+1]\n",
    "        train_y_subset = y_train[:i+1]\n",
    "\n",
    "        # Calculate e_t, e_g, and append to output\n",
    "        training_error, generalization_error, b_hat = noisy_emp_and_gen_risk(\n",
    "            d, train_x_subset, train_y_subset, x_test, y_test)\n",
    "        train_error_arr.append(training_error)\n",
    "        test_error_arr.append(generalization_error)\n",
    "\n",
    "    return train_error_arr, test_error_arr, b_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef generate(x, d):\\n    return np.array([x**i for i in range(d+1)])\\n\\n\\n# Generate a helper variable\\nn = list(range(N))\\nx = np.linspace(0, 1, N)\\n\\n# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 2\\ntrain_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\\n    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 5\\ntrain_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\\n    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 10\\ntrain_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\\n    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\nfunc_g = coef @ generate(x, d)\\nfunc_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\\nfunc_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\\nfunc_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\\n\\n# B_hat predictions given N data points\\nplt.figure(figsize=(15, 8))\\nplt.scatter(X_train_noise[:N], y_train_noise[:N])\\nplt.plot(x, func_g)\\nplt.plot(x, func_b_hat_2)\\nplt.plot(x, func_b_hat_5)\\nplt.plot(x, func_b_hat_10)\\n\\nplt.legend(labels=[r\\'$f_{g(x)}$\\', r\\'$f_{\\\\hat{b}(x; d=2)}$\\',\\n           r\\'$f_{\\\\hat{b}(x; d=5)}$\\', r\\'$f_{\\\\hat{b}(x; d=10)}$\\', \\'Training Data\\'])\\nplt.title(\\n    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\\nplt.savefig(\\'generated_data//loss_over_N\\')\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 10\n",
    "N = 200\n",
    "alpha = 0.05\n",
    "coef = get_d(d)\n",
    "\n",
    "print(d)\n",
    "print(N)\n",
    "os.makedirs('generated_data', exist_ok=True)\n",
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "\n",
    "np.savetxt('generated_data//df_X_train.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_train.csv',\n",
    "           y_train_noise, delimiter=',')\n",
    "np.savetxt('generated_data//df_X_test.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_test.csv',\n",
    "           y_test_noise, delimiter=',')\n",
    "np.savetxt('generated_data//df_weights.csv',\n",
    "           coef.reshape([1, d+1]), delimiter=',')\n",
    "\n",
    "'''\n",
    "def generate(x, d):\n",
    "    return np.array([x**i for i in range(d+1)])\n",
    "\n",
    "\n",
    "# Generate a helper variable\n",
    "n = list(range(N))\n",
    "x = np.linspace(0, 1, N)\n",
    "\n",
    "# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 2\n",
    "train_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\n",
    "    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 5\n",
    "train_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\n",
    "    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 10\n",
    "train_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\n",
    "    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "func_g = coef @ generate(x, d)\n",
    "func_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\n",
    "func_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\n",
    "func_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\n",
    "\n",
    "# B_hat predictions given N data points\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(X_train_noise[:N], y_train_noise[:N])\n",
    "plt.plot(x, func_g)\n",
    "plt.plot(x, func_b_hat_2)\n",
    "plt.plot(x, func_b_hat_5)\n",
    "plt.plot(x, func_b_hat_10)\n",
    "\n",
    "plt.legend(labels=[r'$f_{g(x)}$', r'$f_{\\hat{b}(x; d=2)}$',\n",
    "           r'$f_{\\hat{b}(x; d=5)}$', r'$f_{\\hat{b}(x; d=10)}$', 'Training Data'])\n",
    "plt.title(\n",
    "    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\n",
    "plt.savefig('generated_data//loss_over_N')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    assert len(preds) == len(y), print(\"Dimensions don't match for preds and y - \", preds.shape, y.shape)\n",
    "    loss = (1.0/m) * ((preds-y).T @ (preds - y))  # * np.sum((preds - y)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    grad = (2.0/m) * ((preds - y) @ X)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    fn_gradient = compute_square_loss_gradient(\n",
    "        X, y, theta)  # The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    # Initialize the gradient we approximate\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    e = [np.zeros(num_features) for i in range(num_features)]\n",
    "    for i in range(num_features):\n",
    "        e[i][i] = 1\n",
    "    approx = []\n",
    "    for ei in e:\n",
    "        upper = compute_square_loss(X, y, theta + epsilon*ei)\n",
    "        lower = compute_square_loss(X, y, theta - epsilon*ei)\n",
    "        approx.append((upper - lower)/(2.0 * epsilon))\n",
    "\n",
    "    approx = np.asarray(approx)\n",
    "    if np.linalg.norm(fn_gradient - approx) > tolerance:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  # Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  # Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  # Initialize theta\n",
    "\n",
    "    for step in range(num_step):\n",
    "        theta_hist[step] = theta\n",
    "        loss_hist[step] = compute_square_loss(X, y, theta)\n",
    "        gradient = compute_square_loss_gradient(X, y, theta)\n",
    "\n",
    "        if grad_check:\n",
    "            if not grad_checker(X, y, theta):\n",
    "                sys.exit(\"ERROR: INCORRECT GRADIENT\")\n",
    "            else:\n",
    "                sys.stdout.write(\"GRAD CHECK PASSED\\n\")\n",
    "\n",
    "        theta -= alpha*gradient\n",
    "\n",
    "    # Off by one at end of arrays so recompute\n",
    "    theta_hist[step+1] = theta\n",
    "    loss_hist[step+1] = compute_square_loss(X, y, theta)\n",
    "\n",
    "    return theta_hist, loss_hist\n",
    "\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "\tassert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "\tpreds = X @ theta\n",
    "\tm = len(y)\n",
    "\tgrad = ((2.0/m) * ((preds - y) @ X)) + (2 * lambda_reg * theta)\n",
    "\treturn grad\n",
    "\n",
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "\tnum_instances, num_features = X.shape[0], X.shape[1]\n",
    "\ttheta = np.zeros(num_features) #Initialize theta\n",
    "\ttheta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "\tloss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "\n",
    "\tfor i in range(num_step+1):\n",
    "\t\ttheta_hist[i] = theta\n",
    "\t\tloss_hist[i] = compute_square_loss(X, y, theta)\n",
    "\t\tif i == num_step:\n",
    "\t\t\tbreak\n",
    "\t\tgrad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\treturn loss_hist, theta_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(Xd_train, y_train_noise, alpha=alpha, num_step=500, grad_check=False)\n",
    "\n",
    "loss_test = []\n",
    "for t in range(len(theta_hist)):\n",
    "    loss_test.append(compute_square_loss(Xd_test, y_test_noise, theta_hist[t]))\n",
    "plt.plot(loss_test, label='test')\n",
    "plt.plot(loss_hist, label='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Test and Train Square Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.savefig(\"generated_data/loss_batch_py.jpg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7f43726a79b79bb18500e3546bed249e1c1aa16f3227c093295a5c944512c00"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
