{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "destination = \"../generated_data\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given functions\n",
    "def get_d(deg_true):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "\n",
    "    Returns:\n",
    "    a: (np array of size (deg_true + 1)) coefficients of polynomial g\n",
    "    \"\"\"\n",
    "    return 5 * np.random.randn(deg_true + 1)\n",
    "\n",
    "\n",
    "def get_design_mat(x, deg):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x: (np.array of size N)\n",
    "    deg: (int) max degree used to generate the design matrix\n",
    "\n",
    "    Returns:\n",
    "    X: (np.array of size N x (deg_true + 1)) design matrix\n",
    "    \"\"\"\n",
    "    X = np.array([x ** i for i in range(deg + 1)]).T\n",
    "    return X\n",
    "\n",
    "\n",
    "def draw_sample(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.sort(np.random.rand(N))\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def draw_sample_with_noise(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.random.rand(N)\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a + np.random.randn(N)\n",
    "    return x, y\n",
    "\n",
    "# Define our least squares estimator function\n",
    "\n",
    "\n",
    "def least_squares_estimator(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "\n",
    "    Returns:\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    \"\"\"\n",
    "    # Make sure N > d\n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        raise ValueError(\"You must have at least as many rows as columns!\")\n",
    "    else:\n",
    "        # Compute the solution for b using the closed form linear algebra solution\n",
    "        b_hat = np.linalg.inv(X.T@X) @ X.T @ y\n",
    "        return b_hat\n",
    "\n",
    "\n",
    "def empirical_risk(X, y, b_hat):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    Returns:\n",
    "    emp_risk: (float) \n",
    "    \"\"\"\n",
    "    # Get # of observations\n",
    "    N = X.shape[0]\n",
    "    # Calculate Predictions\n",
    "    y_hat = X @ b_hat\n",
    "    # Calculate squared errors and then empirical risk\n",
    "    sum_of_squared_errors = sum((y_hat-y)**2)\n",
    "    emp_risk = sum_of_squared_errors / N\n",
    "    emp_risk = emp_risk / 2  # because we have 1/2 in our loss function\n",
    "    return emp_risk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef generate(x, d):\\n    return np.array([x**i for i in range(d+1)])\\n\\n\\n# Generate a helper variable\\nn = list(range(N))\\nx = np.linspace(0, 1, N)\\n\\n# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 2\\ntrain_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\\n    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 5\\ntrain_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\\n    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 10\\ntrain_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\\n    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\nfunc_g = coef @ generate(x, d)\\nfunc_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\\nfunc_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\\nfunc_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\\n\\n# B_hat predictions given N data points\\nplt.figure(figsize=(15, 8))\\nplt.scatter(X_train_noise[:N], y_train_noise[:N])\\nplt.plot(x, func_g)\\nplt.plot(x, func_b_hat_2)\\nplt.plot(x, func_b_hat_5)\\nplt.plot(x, func_b_hat_10)\\n\\nplt.legend(labels=[r\\'$f_{g(x)}$\\', r\\'$f_{\\\\hat{b}(x; d=2)}$\\',\\n           r\\'$f_{\\\\hat{b}(x; d=5)}$\\', r\\'$f_{\\\\hat{b}(x; d=10)}$\\', \\'Training Data\\'])\\nplt.title(\\n    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\\nplt.savefig(\\'generated_data//loss_over_N\\')\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 10\n",
    "N = 2000\n",
    "alpha = 0.05\n",
    "coef = get_d(d)\n",
    "\n",
    "print(d)\n",
    "print(N)\n",
    "os.makedirs('generated_data', exist_ok=True)\n",
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "Xd_train = Xd_train[:,1:] #they drop bias column\n",
    "Xd_test = Xd_test[:,1:]\n",
    "\n",
    "np.savetxt('generated_data//df_X_train.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_train.csv',\n",
    "           y_train_noise, delimiter=',')\n",
    "np.savetxt('generated_data//df_X_test.csv', Xd_test, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_test.csv',\n",
    "           y_test_noise, delimiter=',')\n",
    "np.savetxt('generated_data//df_weights.csv',\n",
    "           coef.reshape([1, d+1]), delimiter=',')\n",
    "\n",
    "'''\n",
    "def generate(x, d):\n",
    "    return np.array([x**i for i in range(d+1)])\n",
    "\n",
    "\n",
    "# Generate a helper variable\n",
    "n = list(range(N))\n",
    "x = np.linspace(0, 1, N)\n",
    "\n",
    "# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 2\n",
    "train_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\n",
    "    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 5\n",
    "train_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\n",
    "    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 10\n",
    "train_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\n",
    "    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "func_g = coef @ generate(x, d)\n",
    "func_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\n",
    "func_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\n",
    "func_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\n",
    "\n",
    "# B_hat predictions given N data points\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(X_train_noise[:N], y_train_noise[:N])\n",
    "plt.plot(x, func_g)\n",
    "plt.plot(x, func_b_hat_2)\n",
    "plt.plot(x, func_b_hat_5)\n",
    "plt.plot(x, func_b_hat_10)\n",
    "\n",
    "plt.legend(labels=[r'$f_{g(x)}$', r'$f_{\\hat{b}(x; d=2)}$',\n",
    "           r'$f_{\\hat{b}(x; d=5)}$', r'$f_{\\hat{b}(x; d=10)}$', 'Training Data'])\n",
    "plt.title(\n",
    "    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\n",
    "plt.savefig('generated_data//loss_over_N')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    assert len(preds) == len(y), print(\"Dimensions don't match for preds and y - \", preds.shape, y.shape)\n",
    "    loss = (1.0/m) * ((preds-y).T @ (preds - y))  # * np.sum((preds - y)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    grad = (2.0/m) * ((preds - y) @ X)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    fn_gradient = compute_square_loss_gradient(\n",
    "        X, y, theta)  # The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    # Initialize the gradient we approximate\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    e = [np.zeros(num_features) for i in range(num_features)]\n",
    "    for i in range(num_features):\n",
    "        e[i][i] = 1\n",
    "    approx = []\n",
    "    for ei in e:\n",
    "        upper = compute_square_loss(X, y, theta + epsilon*ei)\n",
    "        lower = compute_square_loss(X, y, theta - epsilon*ei)\n",
    "        approx.append((upper - lower)/(2.0 * epsilon))\n",
    "\n",
    "    approx = np.asarray(approx)\n",
    "    if np.linalg.norm(fn_gradient - approx) > tolerance:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  # Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  # Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  # Initialize theta\n",
    "\n",
    "    for step in range(num_step):\n",
    "        theta_hist[step] = theta\n",
    "        loss_hist[step] = compute_square_loss(X, y, theta)\n",
    "        gradient = compute_square_loss_gradient(X, y, theta)\n",
    "\n",
    "        if grad_check:\n",
    "            if not grad_checker(X, y, theta):\n",
    "                sys.exit(\"ERROR: INCORRECT GRADIENT\")\n",
    "            else:\n",
    "                sys.stdout.write(\"GRAD CHECK PASSED\\n\")\n",
    "\n",
    "        theta -= alpha*gradient\n",
    "\n",
    "    # Off by one at end of arrays so recompute\n",
    "    theta_hist[step+1] = theta\n",
    "    loss_hist[step+1] = compute_square_loss(X, y, theta)\n",
    "\n",
    "    return theta_hist, loss_hist\n",
    "\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "\tassert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "\tpreds = X @ theta\n",
    "\tm = len(y)\n",
    "\tgrad = ((2.0/m) * ((preds - y) @ X)) + (2 * lambda_reg * theta)\n",
    "\treturn grad\n",
    "\n",
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "\tnum_instances, num_features = X.shape[0], X.shape[1]\n",
    "\ttheta = np.zeros(num_features) #Initialize theta\n",
    "\ttheta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "\tloss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "\n",
    "\tfor i in range(num_step+1):\n",
    "\t\ttheta_hist[i] = theta\n",
    "\t\tloss_hist[i] = compute_square_loss(X, y, theta)\n",
    "\t\tif i == num_step:\n",
    "\t\t\tbreak\n",
    "\t\tgrad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\treturn loss_hist, theta_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(Xd_train, y_train_noise, alpha=alpha, num_step=500, grad_check=False)\n",
    "\n",
    "loss_test = []\n",
    "for t in range(len(theta_hist)):\n",
    "    loss_test.append(compute_square_loss(Xd_test, y_test_noise, theta_hist[t]))\n",
    "plt.plot(loss_test, label='test')\n",
    "plt.plot(loss_hist, label='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Test and Train Square Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.savefig(\"generated_data/loss_batch_py.jpg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7f43726a79b79bb18500e3546bed249e1c1aa16f3227c093295a5c944512c00"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
