{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "destination = \"../generated_data\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given functions\n",
    "def get_d(deg_true):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "\n",
    "    Returns:\n",
    "    a: (np array of size (deg_true + 1)) coefficients of polynomial g\n",
    "    \"\"\"\n",
    "    return 5 * np.random.randn(deg_true + 1)\n",
    "\n",
    "\n",
    "def get_design_mat(x, deg):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x: (np.array of size N)\n",
    "    deg: (int) max degree used to generate the design matrix\n",
    "\n",
    "    Returns:\n",
    "    X: (np.array of size N x (deg_true + 1)) design matrix\n",
    "    \"\"\"\n",
    "    X = np.array([x ** i for i in range(deg + 1)]).T\n",
    "    return X\n",
    "\n",
    "\n",
    "def draw_sample(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.sort(np.random.rand(N))\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def draw_sample_with_noise(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.random.rand(N)\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a + np.random.randn(N)\n",
    "    return x, y\n",
    "\n",
    "# Define our least squares estimator function\n",
    "\n",
    "\n",
    "def least_squares_estimator(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "\n",
    "    Returns:\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    \"\"\"\n",
    "    # Make sure N > d\n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        raise ValueError(\"You must have at least as many rows as columns!\")\n",
    "    else:\n",
    "        # Compute the solution for b using the closed form linear algebra solution\n",
    "        b_hat = np.linalg.inv(X.T@X) @ X.T @ y\n",
    "        return b_hat\n",
    "\n",
    "\n",
    "def empirical_risk(X, y, b_hat):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    Returns:\n",
    "    emp_risk: (float) \n",
    "    \"\"\"\n",
    "    # Get # of observations\n",
    "    N = X.shape[0]\n",
    "    # Calculate Predictions\n",
    "    y_hat = X @ b_hat\n",
    "    # Calculate squared errors and then empirical risk\n",
    "    sum_of_squared_errors = sum((y_hat-y)**2)\n",
    "    emp_risk = sum_of_squared_errors / N\n",
    "    emp_risk = emp_risk / 2  # because we have 1/2 in our loss function\n",
    "    return emp_risk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef generate(x, d):\\n    return np.array([x**i for i in range(d+1)])\\n\\n\\n# Generate a helper variable\\nn = list(range(N))\\nx = np.linspace(0, 1, N)\\n\\n# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 2\\ntrain_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\\n    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 5\\ntrain_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\\n    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 10\\ntrain_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\\n    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\nfunc_g = coef @ generate(x, d)\\nfunc_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\\nfunc_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\\nfunc_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\\n\\n# B_hat predictions given N data points\\nplt.figure(figsize=(15, 8))\\nplt.scatter(X_train_noise[:N], y_train_noise[:N])\\nplt.plot(x, func_g)\\nplt.plot(x, func_b_hat_2)\\nplt.plot(x, func_b_hat_5)\\nplt.plot(x, func_b_hat_10)\\n\\nplt.legend(labels=[r\\'$f_{g(x)}$\\', r\\'$f_{\\\\hat{b}(x; d=2)}$\\',\\n           r\\'$f_{\\\\hat{b}(x; d=5)}$\\', r\\'$f_{\\\\hat{b}(x; d=10)}$\\', \\'Training Data\\'])\\nplt.title(\\n    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\\nplt.savefig(\\'generated_data//loss_over_N\\')\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 10\n",
    "N = 100000\n",
    "alpha = 0.05\n",
    "coef = get_d(d)\n",
    "\n",
    "print(d)\n",
    "print(N)\n",
    "os.makedirs('generated_data', exist_ok=True)\n",
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "Xd_train = Xd_train[:,1:] #they drop bias column\n",
    "Xd_test = Xd_test[:,1:]\n",
    "\n",
    "np.savetxt('generated_data//df_X_train_100k.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_train_100k.csv',\n",
    "           y_train_noise, delimiter=',')\n",
    "np.savetxt('generated_data//df_X_test_100k.csv', Xd_test, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_test_100k.csv',\n",
    "           y_test_noise, delimiter=',')\n",
    "\n",
    "\n",
    "'''\n",
    "def generate(x, d):\n",
    "    return np.array([x**i for i in range(d+1)])\n",
    "\n",
    "\n",
    "# Generate a helper variable\n",
    "n = list(range(N))\n",
    "x = np.linspace(0, 1, N)\n",
    "\n",
    "# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 2\n",
    "train_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\n",
    "    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 5\n",
    "train_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\n",
    "    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 10\n",
    "train_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\n",
    "    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "func_g = coef @ generate(x, d)\n",
    "func_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\n",
    "func_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\n",
    "func_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\n",
    "\n",
    "# B_hat predictions given N data points\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(X_train_noise[:N], y_train_noise[:N])\n",
    "plt.plot(x, func_g)\n",
    "plt.plot(x, func_b_hat_2)\n",
    "plt.plot(x, func_b_hat_5)\n",
    "plt.plot(x, func_b_hat_10)\n",
    "\n",
    "plt.legend(labels=[r'$f_{g(x)}$', r'$f_{\\hat{b}(x; d=2)}$',\n",
    "           r'$f_{\\hat{b}(x; d=5)}$', r'$f_{\\hat{b}(x; d=10)}$', 'Training Data'])\n",
    "plt.title(\n",
    "    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\n",
    "plt.savefig('generated_data//loss_over_N')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    assert len(preds) == len(y), print(\"Dimensions don't match for preds and y - \", preds.shape, y.shape)\n",
    "    loss = (1.0/m) * ((preds-y).T @ (preds - y))  # * np.sum((preds - y)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    grad = (2.0/m) * ((preds - y) @ X)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    fn_gradient = compute_square_loss_gradient(\n",
    "        X, y, theta)  # The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    # Initialize the gradient we approximate\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    e = [np.zeros(num_features) for i in range(num_features)]\n",
    "    for i in range(num_features):\n",
    "        e[i][i] = 1\n",
    "    approx = []\n",
    "    for ei in e:\n",
    "        upper = compute_square_loss(X, y, theta + epsilon*ei)\n",
    "        lower = compute_square_loss(X, y, theta - epsilon*ei)\n",
    "        approx.append((upper - lower)/(2.0 * epsilon))\n",
    "\n",
    "    approx = np.asarray(approx)\n",
    "    if np.linalg.norm(fn_gradient - approx) > tolerance:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  # Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  # Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  # Initialize theta\n",
    "\n",
    "    for step in range(num_step):\n",
    "        theta_hist[step] = theta\n",
    "        loss_hist[step] = compute_square_loss(X, y, theta)\n",
    "        gradient = compute_square_loss_gradient(X, y, theta)\n",
    "\n",
    "        if grad_check:\n",
    "            if not grad_checker(X, y, theta):\n",
    "                sys.exit(\"ERROR: INCORRECT GRADIENT\")\n",
    "            else:\n",
    "                sys.stdout.write(\"GRAD CHECK PASSED\\n\")\n",
    "\n",
    "        theta -= alpha*gradient\n",
    "\n",
    "    # Off by one at end of arrays so recompute\n",
    "    theta_hist[step+1] = theta\n",
    "    loss_hist[step+1] = compute_square_loss(X, y, theta)\n",
    "\n",
    "    return theta_hist, loss_hist\n",
    "\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "\tassert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "\tpreds = X @ theta\n",
    "\tm = len(y)\n",
    "\tgrad = ((2.0/m) * ((preds - y) @ X)) + (2 * lambda_reg * theta)\n",
    "\treturn grad\n",
    "\n",
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "\tnum_instances, num_features = X.shape[0], X.shape[1]\n",
    "\ttheta = np.zeros(num_features) #Initialize theta\n",
    "\ttheta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "\tloss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "\n",
    "\tfor i in range(num_step+1):\n",
    "\t\ttheta_hist[i] = theta\n",
    "\t\tloss_hist[i] = compute_square_loss(X, y, theta)\n",
    "\t\tif i == num_step:\n",
    "\t\t\tbreak\n",
    "\t\tgrad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\treturn loss_hist, theta_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(Xd_train, y_train_noise, alpha=alpha, num_step=500, grad_check=False)\n",
    "\n",
    "loss_test = []\n",
    "for t in range(len(theta_hist)):\n",
    "    loss_test.append(compute_square_loss(Xd_test, y_test_noise, theta_hist[t]))\n",
    "plt.plot(loss_test, label='test')\n",
    "plt.plot(loss_hist, label='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Test and Train Square Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.savefig(\"generated_data/loss_batch_py.jpg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7f43726a79b79bb18500e3546bed249e1c1aa16f3227c093295a5c944512c00"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
