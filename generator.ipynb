{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "destination = \"../generated_data\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given functions\n",
    "def get_d(deg_true):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "\n",
    "    Returns:\n",
    "    a: (np array of size (deg_true + 1)) coefficients of polynomial g\n",
    "    \"\"\"\n",
    "    return 5 * np.random.randn(deg_true + 1)\n",
    "\n",
    "\n",
    "def get_design_mat(x, deg):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x: (np.array of size N)\n",
    "    deg: (int) max degree used to generate the design matrix\n",
    "\n",
    "    Returns:\n",
    "    X: (np.array of size N x (deg_true + 1)) design matrix\n",
    "    \"\"\"\n",
    "    X = np.array([x ** i for i in range(deg + 1)]).T\n",
    "    return X\n",
    "\n",
    "\n",
    "def draw_sample(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.sort(np.random.rand(N))\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def draw_sample_with_noise(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.random.rand(N)\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a + np.random.randn(N)\n",
    "    return x, y\n",
    "\n",
    "# Define our least squares estimator function\n",
    "\n",
    "\n",
    "def least_squares_estimator(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "\n",
    "    Returns:\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    \"\"\"\n",
    "    # Make sure N > d\n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        raise ValueError(\"You must have at least as many rows as columns!\")\n",
    "    else:\n",
    "        # Compute the solution for b using the closed form linear algebra solution\n",
    "        b_hat = np.linalg.inv(X.T@X) @ X.T @ y\n",
    "        return b_hat\n",
    "\n",
    "\n",
    "def empirical_risk(X, y, b_hat):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    Returns:\n",
    "    emp_risk: (float) \n",
    "    \"\"\"\n",
    "    # Get # of observations\n",
    "    N = X.shape[0]\n",
    "    # Calculate Predictions\n",
    "    y_hat = X @ b_hat\n",
    "    # Calculate squared errors and then empirical risk\n",
    "    sum_of_squared_errors = sum((y_hat-y)**2)\n",
    "    emp_risk = sum_of_squared_errors / N\n",
    "    emp_risk = emp_risk / 2  # because we have 1/2 in our loss function\n",
    "    return emp_risk\n",
    "\n",
    "\n",
    "def noisy_emp_and_gen_risk(d, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    d: (int) degree of polynomial desired\n",
    "    n: (int) number of samples to be generated in \n",
    "\n",
    "    Outputs:\n",
    "    training_error: (float) average sum of squares of loss function on training data\n",
    "    generalization_error: (float) average Sum of Squares of loss function on test data\n",
    "    \"\"\"\n",
    "    # Generate design matrices\n",
    "    X_train = get_design_mat(x_train, d)\n",
    "    X_test = get_design_mat(x_test, d)\n",
    "\n",
    "    # Calculate b_hat\n",
    "    b_hat = least_squares_estimator(X_train, y_train)\n",
    "    training_error = empirical_risk(X_train, y_train, b_hat)\n",
    "    generalization_error = empirical_risk(X_test, y_test, b_hat)\n",
    "\n",
    "    return training_error, generalization_error, b_hat\n",
    "\n",
    "\n",
    "def poly_risk_gen(d, n, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Ouptut:\n",
    "    train_error_arr: (np.array) e_t for various n\n",
    "    test_error_arr: (np.array) e_g for various n\n",
    "    \"\"\"\n",
    "    # Initiliaze np arrays\n",
    "    train_error_arr = []\n",
    "    test_error_arr = []\n",
    "\n",
    "    # Iterate over N\n",
    "    for i in range(d+1, len(n)):\n",
    "\n",
    "        # Get relevant subset of data\n",
    "        train_x_subset = x_train[:i+1]\n",
    "        train_y_subset = y_train[:i+1]\n",
    "\n",
    "        # Calculate e_t, e_g, and append to output\n",
    "        training_error, generalization_error, b_hat = noisy_emp_and_gen_risk(\n",
    "            d, train_x_subset, train_y_subset, x_test, y_test)\n",
    "        train_error_arr.append(training_error)\n",
    "        test_error_arr.append(generalization_error)\n",
    "\n",
    "    return train_error_arr, test_error_arr, b_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values at Index 0 For Vectors b_Hat and coef 2.3542244245506003 2.3542244245508055\n",
      "Difference rounded to 5 decimal places: -0.0\n",
      "Values at Index 1 For Vectors b_Hat and coef 2.2613576818054786 2.261357681806302\n",
      "Difference rounded to 5 decimal places: -0.0\n",
      "Values at Index 2 For Vectors b_Hat and coef 0.0534788781975877 0.05347887819645081\n",
      "Difference rounded to 5 decimal places: 0.0\n",
      "Values at Index 3 For Vectors b_Hat and coef 5.395578128426202 5.395578128426528\n",
      "Difference rounded to 5 decimal places: -0.0\n"
     ]
    }
   ],
   "source": [
    "# Set the degree of the polynomial\n",
    "d = 3\n",
    "\n",
    "# Return coefficients\n",
    "coef = get_d(d)\n",
    "\n",
    "# Generate training and test data -> N_train is the size of the train sample to draw, N_test for test\n",
    "N_train = 100\n",
    "N_test = 1000\n",
    "\n",
    "X_train, y_train = draw_sample(d, coef, N_train)\n",
    "X_test, y_test = draw_sample(d, coef, N_test)\n",
    "\n",
    "# Generate design matrices\n",
    "Xd_train = get_design_mat(X_train, d)\n",
    "Xd_test = get_design_mat(X_test, d)\n",
    "\n",
    "b_hat = least_squares_estimator(Xd_train, y_train)\n",
    "\n",
    "# Compare coef and b_hat values (should be same as we provide the data-generating distribution)\n",
    "for i in range(len(b_hat)):\n",
    "    print('Values at Index', i,\n",
    "          'For Vectors b_Hat and coef', coef[i], b_hat[i])\n",
    "    print(\"Difference rounded to 5 decimal places:\",\n",
    "          np.round(coef[i]-b_hat[i], 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef generate(x, d):\\n    return np.array([x**i for i in range(d+1)])\\n\\n\\n# Generate a helper variable\\nn = list(range(N))\\nx = np.linspace(0, 1, N)\\n\\n# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 2\\ntrain_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\\n    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 5\\ntrain_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\\n    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 10\\ntrain_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\\n    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\nfunc_g = coef @ generate(x, d)\\nfunc_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\\nfunc_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\\nfunc_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\\n\\n# B_hat predictions given N data points\\nplt.figure(figsize=(15, 8))\\nplt.scatter(X_train_noise[:N], y_train_noise[:N])\\nplt.plot(x, func_g)\\nplt.plot(x, func_b_hat_2)\\nplt.plot(x, func_b_hat_5)\\nplt.plot(x, func_b_hat_10)\\n\\nplt.legend(labels=[r\\'$f_{g(x)}$\\', r\\'$f_{\\\\hat{b}(x; d=2)}$\\',\\n           r\\'$f_{\\\\hat{b}(x; d=5)}$\\', r\\'$f_{\\\\hat{b}(x; d=10)}$\\', \\'Training Data\\'])\\nplt.title(\\n    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\\nplt.savefig(\\'generated_data//loss_over_N\\')\\n'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 10\n",
    "N = 200\n",
    "alpha = 0.05\n",
    "coef = get_d(d)\n",
    "\n",
    "print(d)\n",
    "print(N)\n",
    "os.makedirs('generated_data', exist_ok=True)\n",
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "\n",
    "np.savetxt('generated_data//df_X_train.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_train.csv',\n",
    "           y_train_noise.reshape([1, N]), delimiter=',')\n",
    "np.savetxt('generated_data//df_X_test.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_test.csv',\n",
    "           y_test_noise.reshape([1, N]), delimiter=',')\n",
    "np.savetxt('generated_data//df_weights.csv',\n",
    "           coef.reshape([1, d+1]), delimiter=',')\n",
    "\n",
    "'''\n",
    "def generate(x, d):\n",
    "    return np.array([x**i for i in range(d+1)])\n",
    "\n",
    "\n",
    "# Generate a helper variable\n",
    "n = list(range(N))\n",
    "x = np.linspace(0, 1, N)\n",
    "\n",
    "# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 2\n",
    "train_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\n",
    "    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 5\n",
    "train_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\n",
    "    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 10\n",
    "train_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\n",
    "    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "func_g = coef @ generate(x, d)\n",
    "func_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\n",
    "func_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\n",
    "func_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\n",
    "\n",
    "# B_hat predictions given N data points\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(X_train_noise[:N], y_train_noise[:N])\n",
    "plt.plot(x, func_g)\n",
    "plt.plot(x, func_b_hat_2)\n",
    "plt.plot(x, func_b_hat_5)\n",
    "plt.plot(x, func_b_hat_10)\n",
    "\n",
    "plt.legend(labels=[r'$f_{g(x)}$', r'$f_{\\hat{b}(x; d=2)}$',\n",
    "           r'$f_{\\hat{b}(x; d=5)}$', r'$f_{\\hat{b}(x; d=10)}$', 'Training Data'])\n",
    "plt.title(\n",
    "    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\n",
    "plt.savefig('generated_data//loss_over_N')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    assert len(preds) == len(y), print(\"Dimensions don't match for preds and y - \", preds.shape, y.shape)\n",
    "    loss = (1.0/m) * ((preds-y).T @ (preds - y))  # * np.sum((preds - y)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    grad = (2.0/m) * ((preds - y) @ X)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    fn_gradient = compute_square_loss_gradient(\n",
    "        X, y, theta)  # The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    # Initialize the gradient we approximate\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    e = [np.zeros(num_features) for i in range(num_features)]\n",
    "    for i in range(num_features):\n",
    "        e[i][i] = 1\n",
    "    approx = []\n",
    "    for ei in e:\n",
    "        upper = compute_square_loss(X, y, theta + epsilon*ei)\n",
    "        lower = compute_square_loss(X, y, theta - epsilon*ei)\n",
    "        approx.append((upper - lower)/(2.0 * epsilon))\n",
    "\n",
    "    approx = np.asarray(approx)\n",
    "    if np.linalg.norm(fn_gradient - approx) > tolerance:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  # Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  # Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  # Initialize theta\n",
    "\n",
    "    for step in range(num_step):\n",
    "        theta_hist[step] = theta\n",
    "        loss_hist[step] = compute_square_loss(X, y, theta)\n",
    "        gradient = compute_square_loss_gradient(X, y, theta)\n",
    "\n",
    "        if grad_check:\n",
    "            if not grad_checker(X, y, theta):\n",
    "                sys.exit(\"ERROR: INCORRECT GRADIENT\")\n",
    "            else:\n",
    "                sys.stdout.write(\"GRAD CHECK PASSED\\n\")\n",
    "\n",
    "        theta -= alpha*gradient\n",
    "\n",
    "    # Off by one at end of arrays so recompute\n",
    "    theta_hist[step+1] = theta\n",
    "    loss_hist[step+1] = compute_square_loss(X, y, theta)\n",
    "\n",
    "    return theta_hist, loss_hist\n",
    "\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "\tassert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "\tpreds = X @ theta\n",
    "\tm = len(y)\n",
    "\tgrad = ((2.0/m) * ((preds - y) @ X)) + (2 * lambda_reg * theta)\n",
    "\treturn grad\n",
    "\n",
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "\tnum_instances, num_features = X.shape[0], X.shape[1]\n",
    "\ttheta = np.zeros(num_features) #Initialize theta\n",
    "\ttheta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "\tloss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "\n",
    "\tfor i in range(num_step+1):\n",
    "\t\ttheta_hist[i] = theta\n",
    "\t\tloss_hist[i] = compute_square_loss(X, y, theta)\n",
    "\t\tif i == num_step:\n",
    "\t\t\tbreak\n",
    "\t\tgrad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\treturn loss_hist, theta_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(Xd_train, y_train_noise, alpha=alpha, num_step=500, grad_check=False)\n",
    "\n",
    "loss_test = []\n",
    "for t in range(len(theta_hist)):\n",
    "    loss_test.append(compute_square_loss(Xd_test, y_test_noise, theta_hist[t]))\n",
    "plt.plot(loss_test, label='test')\n",
    "plt.plot(loss_hist, label='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Test and Train Square Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.savefig(\"generated_data/loss_batch_py.jpg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7f43726a79b79bb18500e3546bed249e1c1aa16f3227c093295a5c944512c00"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
