{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "destination = \"../generated_data\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given functions\n",
    "def get_d(deg_true):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "\n",
    "    Returns:\n",
    "    a: (np array of size (deg_true + 1)) coefficients of polynomial g\n",
    "    \"\"\"\n",
    "    return 5 * np.random.randn(deg_true + 1)\n",
    "\n",
    "\n",
    "def get_design_mat(x, deg):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x: (np.array of size N)\n",
    "    deg: (int) max degree used to generate the design matrix\n",
    "\n",
    "    Returns:\n",
    "    X: (np.array of size N x (deg_true + 1)) design matrix\n",
    "    \"\"\"\n",
    "    X = np.array([x ** i for i in range(deg + 1)]).T\n",
    "    return X\n",
    "\n",
    "\n",
    "def draw_sample(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.sort(np.random.rand(N))\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def draw_sample_with_noise(deg_true, a, N):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "\n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"\n",
    "    x = np.random.rand(N)\n",
    "    X = get_design_mat(x, deg_true)\n",
    "    y = X @ a + np.random.randn(N)\n",
    "    return x, y\n",
    "\n",
    "# Define our least squares estimator function\n",
    "\n",
    "\n",
    "def least_squares_estimator(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "\n",
    "    Returns:\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    \"\"\"\n",
    "    # Make sure N > d\n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        raise ValueError(\"You must have at least as many rows as columns!\")\n",
    "    else:\n",
    "        # Compute the solution for b using the closed form linear algebra solution\n",
    "        b_hat = np.linalg.inv(X.T@X) @ X.T @ y\n",
    "        return b_hat\n",
    "\n",
    "\n",
    "def empirical_risk(X, y, b_hat):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X: (np.matrix of size N x (deg_true +1))\n",
    "    y: (np.array) of size deg_true + 1 x 1\n",
    "    b_hat: (np.array) of size N x (deg_true + 1)\n",
    "    Returns:\n",
    "    emp_risk: (float) \n",
    "    \"\"\"\n",
    "    # Get # of observations\n",
    "    N = X.shape[0]\n",
    "    # Calculate Predictions\n",
    "    y_hat = X @ b_hat\n",
    "    # Calculate squared errors and then empirical risk\n",
    "    sum_of_squared_errors = sum((y_hat-y)**2)\n",
    "    emp_risk = sum_of_squared_errors / N\n",
    "    emp_risk = emp_risk / 2  # because we have 1/2 in our loss function\n",
    "    return emp_risk\n",
    "\n",
    "\n",
    "def noisy_emp_and_gen_risk(d, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    d: (int) degree of polynomial desired\n",
    "    n: (int) number of samples to be generated in \n",
    "\n",
    "    Outputs:\n",
    "    training_error: (float) average sum of squares of loss function on training data\n",
    "    generalization_error: (float) average Sum of Squares of loss function on test data\n",
    "    \"\"\"\n",
    "    # Generate design matrices\n",
    "    X_train = get_design_mat(x_train, d)\n",
    "    X_test = get_design_mat(x_test, d)\n",
    "\n",
    "    # Calculate b_hat\n",
    "    b_hat = least_squares_estimator(X_train, y_train)\n",
    "    training_error = empirical_risk(X_train, y_train, b_hat)\n",
    "    generalization_error = empirical_risk(X_test, y_test, b_hat)\n",
    "\n",
    "    return training_error, generalization_error, b_hat\n",
    "\n",
    "\n",
    "def poly_risk_gen(d, n, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Ouptut:\n",
    "    train_error_arr: (np.array) e_t for various n\n",
    "    test_error_arr: (np.array) e_g for various n\n",
    "    \"\"\"\n",
    "    # Initiliaze np arrays\n",
    "    train_error_arr = []\n",
    "    test_error_arr = []\n",
    "\n",
    "    # Iterate over N\n",
    "    for i in range(d+1, len(n)):\n",
    "\n",
    "        # Get relevant subset of data\n",
    "        train_x_subset = x_train[:i+1]\n",
    "        train_y_subset = y_train[:i+1]\n",
    "\n",
    "        # Calculate e_t, e_g, and append to output\n",
    "        training_error, generalization_error, b_hat = noisy_emp_and_gen_risk(\n",
    "            d, train_x_subset, train_y_subset, x_test, y_test)\n",
    "        train_error_arr.append(training_error)\n",
    "        test_error_arr.append(generalization_error)\n",
    "\n",
    "    return train_error_arr, test_error_arr, b_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values at Index 0 For Vectors b_Hat and coef 0.5334260207324824 0.5334260207324186\n",
      "Difference rounded to 5 decimal places: 0.0\n",
      "Values at Index 1 For Vectors b_Hat and coef 2.6991543980305117 2.6991543980299606\n",
      "Difference rounded to 5 decimal places: 0.0\n",
      "Values at Index 2 For Vectors b_Hat and coef -2.258233654861868 -2.258233654861245\n",
      "Difference rounded to 5 decimal places: -0.0\n",
      "Values at Index 3 For Vectors b_Hat and coef 3.370161954632771 3.3701619546329695\n",
      "Difference rounded to 5 decimal places: -0.0\n"
     ]
    }
   ],
   "source": [
    "# Set the degree of the polynomial\n",
    "d = 3\n",
    "\n",
    "# Return coefficients\n",
    "coef = get_d(d)\n",
    "\n",
    "# Generate training and test data -> N_train is the size of the train sample to draw, N_test for test\n",
    "N_train = 100\n",
    "N_test = 1000\n",
    "\n",
    "X_train, y_train = draw_sample(d, coef, N_train)\n",
    "X_test, y_test = draw_sample(d, coef, N_test)\n",
    "\n",
    "# Generate design matrices\n",
    "Xd_train = get_design_mat(X_train, d)\n",
    "Xd_test = get_design_mat(X_test, d)\n",
    "\n",
    "b_hat = least_squares_estimator(Xd_train, y_train)\n",
    "\n",
    "# Compare coef and b_hat values (should be same as we provide the data-generating distribution)\n",
    "for i in range(len(b_hat)):\n",
    "    print('Values at Index', i,\n",
    "          'For Vectors b_Hat and coef', coef[i], b_hat[i])\n",
    "    print(\"Difference rounded to 5 decimal places:\",\n",
    "          np.round(coef[i]-b_hat[i], 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef generate(x, d):\\n    return np.array([x**i for i in range(d+1)])\\n\\n\\n# Generate a helper variable\\nn = list(range(N))\\nx = np.linspace(0, 1, N)\\n\\n# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 2\\ntrain_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\\n    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 5\\ntrain_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\\n    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\n\\n# Calculate e_g, e_t for various N\\'s of polynomial degree 10\\ntrain_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\\n    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\\nfunc_g = coef @ generate(x, d)\\nfunc_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\\nfunc_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\\nfunc_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\\n\\n# B_hat predictions given N data points\\nplt.figure(figsize=(15, 8))\\nplt.scatter(X_train_noise[:N], y_train_noise[:N])\\nplt.plot(x, func_g)\\nplt.plot(x, func_b_hat_2)\\nplt.plot(x, func_b_hat_5)\\nplt.plot(x, func_b_hat_10)\\n\\nplt.legend(labels=[r\\'$f_{g(x)}$\\', r\\'$f_{\\\\hat{b}(x; d=2)}$\\',\\n           r\\'$f_{\\\\hat{b}(x; d=5)}$\\', r\\'$f_{\\\\hat{b}(x; d=10)}$\\', \\'Training Data\\'])\\nplt.title(\\n    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\\nplt.savefig(\\'generated_data//loss_over_N\\')\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 10\n",
    "N = 200\n",
    "alpha = 0.05\n",
    "coef = get_d(d)\n",
    "\n",
    "print(d)\n",
    "print(N)\n",
    "os.makedirs('generated_data', exist_ok=True)\n",
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "\n",
    "np.savetxt('generated_data//df_X_train.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_train.csv',\n",
    "           y_train_noise, delimiter=',')\n",
    "np.savetxt('generated_data//df_X_test.csv', Xd_train, delimiter=',')\n",
    "np.savetxt('generated_data//df_y_test.csv',\n",
    "           y_test_noise, delimiter=',')\n",
    "np.savetxt('generated_data//df_weights.csv',\n",
    "           coef.reshape([1, d+1]), delimiter=',')\n",
    "\n",
    "'''\n",
    "def generate(x, d):\n",
    "    return np.array([x**i for i in range(d+1)])\n",
    "\n",
    "\n",
    "# Generate a helper variable\n",
    "n = list(range(N))\n",
    "x = np.linspace(0, 1, N)\n",
    "\n",
    "# Ok the following requires that you have enough data pts so can just comment out as appropriate (N>=20)\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 2\n",
    "train_error_arr_2, test_error_arr_2, b_hat_2_1000 = poly_risk_gen(\n",
    "    2, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 5\n",
    "train_error_arr_5, test_error_arr_5, b_hat_5_1000 = poly_risk_gen(\n",
    "    5, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "\n",
    "# Calculate e_g, e_t for various N's of polynomial degree 10\n",
    "train_error_arr_10, test_error_arr_10, b_hat_10_1000 = poly_risk_gen(\n",
    "    10, n, X_train_noise, y_train_noise, X_test_noise, y_test_noise)\n",
    "func_g = coef @ generate(x, d)\n",
    "func_b_hat_2 = b_hat_2_1000 @ generate(x, 2)\n",
    "func_b_hat_5 = b_hat_5_1000 @ generate(x, 5)\n",
    "func_b_hat_10 = b_hat_10_1000 @ generate(x, 10)\n",
    "\n",
    "# B_hat predictions given N data points\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(X_train_noise[:N], y_train_noise[:N])\n",
    "plt.plot(x, func_g)\n",
    "plt.plot(x, func_b_hat_2)\n",
    "plt.plot(x, func_b_hat_5)\n",
    "plt.plot(x, func_b_hat_10)\n",
    "\n",
    "plt.legend(labels=[r'$f_{g(x)}$', r'$f_{\\hat{b}(x; d=2)}$',\n",
    "           r'$f_{\\hat{b}(x; d=5)}$', r'$f_{\\hat{b}(x; d=10)}$', 'Training Data'])\n",
    "plt.title(\n",
    "    \"Polynomial Estimation Functions vs Ground Truth and Training Data, N={n}\".format(n=N))\n",
    "plt.savefig('generated_data//loss_over_N')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    assert len(preds) == len(y), print(\"Dimensions don't match for preds and y - \", preds.shape, y.shape)\n",
    "    loss = (1.0/m) * ((preds-y).T @ (preds - y))  # * np.sum((preds - y)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    assert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "    preds = X @ theta\n",
    "    m = len(y)\n",
    "    grad = (2.0/m) * ((preds - y) @ X)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    fn_gradient = compute_square_loss_gradient(\n",
    "        X, y, theta)  # The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    # Initialize the gradient we approximate\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    e = [np.zeros(num_features) for i in range(num_features)]\n",
    "    for i in range(num_features):\n",
    "        e[i][i] = 1\n",
    "    approx = []\n",
    "    for ei in e:\n",
    "        upper = compute_square_loss(X, y, theta + epsilon*ei)\n",
    "        lower = compute_square_loss(X, y, theta - epsilon*ei)\n",
    "        approx.append((upper - lower)/(2.0 * epsilon))\n",
    "\n",
    "    approx = np.asarray(approx)\n",
    "    if np.linalg.norm(fn_gradient - approx) > tolerance:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  # Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  # Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  # Initialize theta\n",
    "\n",
    "    for step in range(num_step):\n",
    "        theta_hist[step] = theta\n",
    "        loss_hist[step] = compute_square_loss(X, y, theta)\n",
    "        gradient = compute_square_loss_gradient(X, y, theta)\n",
    "\n",
    "        if grad_check:\n",
    "            if not grad_checker(X, y, theta):\n",
    "                sys.exit(\"ERROR: INCORRECT GRADIENT\")\n",
    "            else:\n",
    "                sys.stdout.write(\"GRAD CHECK PASSED\\n\")\n",
    "\n",
    "        theta -= alpha*gradient\n",
    "\n",
    "    # Off by one at end of arrays so recompute\n",
    "    theta_hist[step+1] = theta\n",
    "    loss_hist[step+1] = compute_square_loss(X, y, theta)\n",
    "\n",
    "    return theta_hist, loss_hist\n",
    "\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "\tassert X.shape[1] == theta.shape[0], print(\"Dimensions don't match for X and theta - \", X.shape, theta.shape)\n",
    "\tpreds = X @ theta\n",
    "\tm = len(y)\n",
    "\tgrad = ((2.0/m) * ((preds - y) @ X)) + (2 * lambda_reg * theta)\n",
    "\treturn grad\n",
    "\n",
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "\tnum_instances, num_features = X.shape[0], X.shape[1]\n",
    "\ttheta = np.zeros(num_features) #Initialize theta\n",
    "\ttheta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "\tloss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "\n",
    "\tfor i in range(num_step+1):\n",
    "\t\ttheta_hist[i] = theta\n",
    "\t\tloss_hist[i] = compute_square_loss(X, y, theta)\n",
    "\t\tif i == num_step:\n",
    "\t\t\tbreak\n",
    "\t\tgrad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\treturn loss_hist, theta_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_noise, y_train_noise = draw_sample_with_noise(d, coef, N)\n",
    "X_test_noise, y_test_noise = draw_sample_with_noise(d, coef, N)\n",
    "Xd_train = get_design_mat(X_train_noise, d).reshape([N, d+1])\n",
    "Xd_test = get_design_mat(X_test_noise, d).reshape([N, d+1])\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(Xd_train, y_train_noise, alpha=alpha, num_step=500, grad_check=False)\n",
    "\n",
    "loss_test = []\n",
    "for t in range(len(theta_hist)):\n",
    "    loss_test.append(compute_square_loss(Xd_test, y_test_noise, theta_hist[t]))\n",
    "plt.plot(loss_test, label='test')\n",
    "plt.plot(loss_hist, label='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Test and Train Square Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.savefig(\"generated_data/loss_batch_py.jpg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7f43726a79b79bb18500e3546bed249e1c1aa16f3227c093295a5c944512c00"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
